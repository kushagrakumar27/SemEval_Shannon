(Author: Marcus Walker)

Our results take the form of model descriptions and accuracy statistics generated by Weka after running our data through the J48 algorithm, as described in README.txt.  Results are generated for both the episode-delimited and scene-delimited trial data, and are discussed below.

The data fed to the model consist of instances of sequential tokens (words or punctuation) from scripts from the program, "Friends", annotated with features such as literal token, generalized lemma of the token, part of speech, speaker, scene, and episode.  The goal is to classify those instances according to the referent, if applicable.  The class variable, then, is "Entity ID": the unique numerical identification code corresponding to a character from the program.  Entity ID takes on one of 37 possible values, each corresponding to a unique character.

Descriptive Statistics Summary

Episode Delimited Data
Kappa (inter-rater agreement) :				68.5%
Precision (true positives/classified positives) :	69.6%
Recall (% positives classified as positive) :		72.4%

Scene Delimited Data	
Kappa (inter-rater agreement) :				72.9%
Precision (true positives/classified positives) :	73.5%
Recall (% positives classified as positive) :		76.1%


Analysis

The various measures of accuracy are high enough to conclude that something is, indeed, being modeled.  Even without time, sequential, and coreference global context, patterns exist within the dialogues that can be used to establish a firm foundation for prediction.  It's notable that the model, in its current form, consistently scores higher in recall than precision, which suggests that it is erring on the side of the most probable classification in the set.  This is not surprising, but it is helpful to note as we work to tweak and augment the model going forward.

Also notable, the kappa statistics for both models, while expected to always be lower than the accuracy, are not lower than the accuracy by much.  This is a good sign, since it suggests that the bulk of the model's correct predictions are being made based on an actual underlying pattern, and not by brute chance.

Let's take a closer look at the decision trees themselves.  They are much too large to reproduce entirely here in a way that is constructive, but we can describe them and trace several paths through a tree to see how the model arrives at its classifications.

The generated trees are much wider than they are deep; the trees terminate in the neighborhood of 15,000 leaves, but only tend to reach a maximum depth of 4.  This is not entirely unexpected, since the number of features is low, while the number of possible values each feature can take on (especially, for example, the feature describing the literal token) is quite high.  A convenient side effect of this structure is that paths through the tree are quite easy to trace!

The first thing the model wants to know, in any case, is who the Speaker of the token is.  Let's first suppose that the speaker is Joey Tribbiani.  In that case, the model wants to know the Word Form, or the literal token.  Let's suppose that the token is "Angela".  The model classifies the Entity ID as "187", a code corresponding to "Joey's Date".  Straightforward!

Let's suppose, instead, that the speaker is Phoebe Buffay.  The model would then want to know the Lemma, the generalized form of the token.  Let's then suppose that the lemma is "she".  The model then wants to know how early in the episode's script the token appears.  If it is early, the model will classify the Entity ID as "248", corresponding to "Monica Geller".  If it's later in the script, the model will classify the Entity ID as "387", corresponding to "Ursula".  Nice job, model!

Finally, let's consider the case when the speaker is "Bernice".  The model, learning that the speaker is Bernice, doesn't care about anything else and confidently classifies the Entity ID as "306", corresponding to "Rachel Green".  

It is clear that the small size of the trial data is having an influence on the model.  For some speakers, the model is willing to make a classification based just on the name of the speaker, suggesting that the speaker only ever refers to one character, or refers to one character so often that distinguishing further yields no significant decrease in entropy.

On the other hand, branches corresponding to certain speakers indicate what the model might do for most speakers in the larger data set.  For the speaker "Rachel Green", a check is performed on the lemma of the token.  Depending on the lemma, a classification might be made right away.  However, as an example, for the lemma "you", we can see a lot of branching happen.  The episode title and scene ID are checked and, depending on the results there, the model drills down further to the specific wordform or even checks the ordinality of the token.s sentence in sequence.  This suggests that rudimentary global context is already being considered, and could be improved and augmented in large part by the addition of new attributes.  It's easy to imagine even an attribute measuring previous wordform, incorporating a one-step Markov measure, improving performance based on the addition of context.  With the small number of attributes currently in our dataset, it wouldn't be unreasonable to extend that to even a 10-step condition, provided sequence boundaries are accounted for.

Currently, the features of most significance seem to be Speaker, Word Form, and Lemma.  Nearly all of the classification is being done based on those features.

Certain of the values we see at nodes in the tree also suggest that additional data cleaning could improve performance.  It's unlikely that the Scene ID "(/fr" is an intended value, for example.  Cleaning these fields isn't guaranteed to improve performance, but if it hurts performance, it should do so in a way that doesn't affect our Cohen's kappa, which should be considered the most reliable measure of our model's accuracy.

It was our hope to exceed a 50% baseline accuracy.  50% would be the expected accuracy of a random classifier for a maximal-entropy binary system.  While the distribution of classifications in our dataset is not uniform, our classifier must select between not 2, but 400 classifications.  Running our data through even a Naive Bayes classifier, which should outperform random assignment, produced accuracy in the 30% range.  Based on this, we considered 50% a reasonable target to attempt to hit and, indeed, this first and incomplete draft of our model does significantly exceed that baseline.  Huzzah!

