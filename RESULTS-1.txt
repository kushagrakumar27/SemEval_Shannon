(Author: Marcus Walker)

Our results take the form of model descriptions and accuracy statistics generated by Weka after running our data through the J48 algorithm, as described in README.txt.  Results are generated for both the episode-delimited and scene-delimited trial data, and are discussed below.

Descriptive Statistics Summary

Episode Delimited Data
Kappa (inter-rater agreement) :			68.5%
Precision (true positives/classified positives) :	69.6%
Recall (% positives classified as positive) :		72.4%

Scene Delimited Data	
Kappa (inter-rater agreement) :			72.9%
Precision (true positives/classified positives) :	73.5%
Recall (% positives classified as positive) :		76.1%


Analysis

The various measures of accuracy are high enough to conclude that something is, indeed, being modeled.  Even without time, sequential, and coreference global context, patterns exist within the dialogues that can be used to establish a firm foundation for prediction.  It.s notable that the model, in its current form, consistently scores higher in recall than precision, which suggests that it is erring on the side of the most probable classification in the set.  This is not surprising, but it is helpful to note as we work to tweak and augment the model going forward.

Also notable, the kappa statistics for both models, while expected to always be lower than the accuracy, are not lower than the accuracy by much.  This is a good sign, since it suggests that the bulk of the model.s correct predictions are being made based on an actual underlying pattern, and not by brute chance.

Looking more closely at the decision trees themselves, it is clear that the small size of the trial data is having an influence on the model.  For some speakers, the model is willing to make a classification based just on the name of the speaker, suggesting that the speaker only ever refers to one character, or refers to one character so often that distinguishing further yields no significant decrease in entropy.  This is even true for certain of the characters in the main cast, which is very suggestive of a lack of observations in the data.

On the other hand, branches corresponding to certain speakers indicate what the model might do for most speakers in the larger data set.  For the speaker .Rachel Green., a check is performed on the lemma of the token.  Depending on the lemma, a classification might be made right away.  However, as an example, for the lemma .you., we can see a lot of branching happen.  The episode title and scene ID are checked and, depending on the results there, the model drills down further to the specific wordform or even checks the ordinality of the token.s sentence in sequence.  This suggests that rudimentary global context is already being considered, and could be improved and augmented in large part by the addition of new attributes.  It.s easy to imagine even an attribute measuring previous wordform, incorporating a one-step Markov measure, improving performance based on the addition of context.  With the small number of attributes currently in our dataset, it wouldn.t be unreasonable to extend that to even a 10-step condition, provided sequence boundaries are accounted for.

Certain of the values we see at nodes in the tree also suggest that additional data cleaning could improve performance.  It.s unlikely that the Scene ID .(/fr. is an intended value, for example.  Cleaning these fields isn.t guaranteed to improve performance, but if it hurts performance, it should do so in a way that doesn.t affect our Cohen.s kappa, which should be considered the most reliable measure of our model.s accuracy.

It was our hope to exceed a 50% baseline accuracy.  50% would be the expected accuracy of a random classifier for a maximal-entropy binary system.  While the distribution of classifications in our dataset is not uniform, our classifier must select between not 2, but 400 classifications.  Running our data through even a Naive Bayes classifier, which should outperform random assignment, produced accuracy in the 30% range.  Based on this, we considered 50% a reasonable target to attempt to hit and, indeed, this first and incomplete draft of our model does significantly exceed that baseline.  Huzzah!

